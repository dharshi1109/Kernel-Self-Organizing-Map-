{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maha9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "48000 12000\n",
      "Epoch 1/5, Validation Loss: 0.2618\n",
      "Epoch 2/5, Validation Loss: 0.1848\n",
      "Epoch 3/5, Validation Loss: 0.1480\n",
      "Epoch 4/5, Validation Loss: 0.1475\n",
      "Epoch 5/5, Validation Loss: 0.1350\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader,random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "t=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
    "train=datasets.MNIST(root='./data',train=True,download=True,transform=t)\n",
    "test=datasets.MNIST(root='./data',train=False,download=True,transform=t)\n",
    "train.targets[5]\n",
    "\n",
    "total_len=len(train)\n",
    "training_size=int(0.8*total_len)\n",
    "valid_size=int(0.2*total_len)\n",
    "print(training_size,valid_size)\n",
    "trainSet,valSet=random_split(train,[training_size,valid_size])\n",
    "\n",
    "train_loader = DataLoader(trainSet, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(valSet, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False)\n",
    "dataiter = iter(train_loader)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "writer=SummaryWriter()\n",
    "import torch\n",
    "trainloss=[]\n",
    "valoss=[]\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        #print(labels.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            vloss = criterion(outputs, labels)\n",
    "            val_loss +=criterion(outputs, labels).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        writer.add_scalar('loss',val_loss,global_step=epoch)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "    trainloss.append(loss.data)\n",
    "    valoss.append(vloss.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,root_dir,transform=None):\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "        self.file_paths=[]\n",
    "        self.class_names=sorted(os.listdir(root_dir))\n",
    "\n",
    "        for class_name in self.class_names:\n",
    "            class_path=os.path.join(self.root_dir,class_name)\n",
    "            files=[os.path.join(class_path, file) for file in os.listdir(class_path)]\n",
    "            self.file_paths.extend(files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img_path=self.file_paths[idx]\n",
    "        image=Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image=self.transform(image)\n",
    "\n",
    "        class_name=os.path.basename(os.path.dirname(img_path))\n",
    "        label_idx=torch.tensor(self.class_names.index(class_name),dtype=torch.long)\n",
    "\n",
    "        return image,label_idx,class_name\n",
    "    \n",
    "transform=transforms.Compose([transforms.Resize((256,256)),transforms.ToTensor()])\n",
    "dataset=CustomDataset(root_dir=r\"\",transform=transform)\n",
    "print(\"Length of dataset:\",len(dataset))\n",
    "index=1\n",
    "img,label_idx,label_name=dataset[index]\n",
    "import numpy as np\n",
    "img = np.reshape(img, (-1,256,256,3))\n",
    "file_writer = tf.summary.create_file_writer(\"./logs\")\n",
    "with file_writer.as_default():\n",
    "  tf.summary.image(\"Training data\", img, step=0)\n",
    "train_size=int(0.8*len(dataset))\n",
    "test_size=len(dataset)-train_size\n",
    "train_dataset,test_dataset=random_split(dataset,[train_size,test_size])\n",
    "class RBFLayer(torch.nn.Module):\n",
    "    def __init__(self,n,input_dim):\n",
    "        super(RBFLayer,self).__init__()\n",
    "        self.centers=torch.nn.Parameter(torch.randn(n,input_dim))\n",
    "        self.sigma=torch.nn.Parameter(torch.ones(n))\n",
    "    def forward(self,x):\n",
    "        x=x.view(x.size(0),-1)\n",
    "        dist=torch.cdist(x,self.centers)\n",
    "        rbf=torch.exp(-dist.pow(2)/(2*self.sigma.pow(2)))\n",
    "        return rbf\n",
    "class RBFNetwork(torch.nn.Module):\n",
    "    def __init__(self,input_dim,n,od):\n",
    "        super(RBFNetwork,self).__init__()\n",
    "        self.rbflayer=RBFLayer(n,input_dim)\n",
    "        self.linearlayer=torch.nn.Linear(n,od)\n",
    "    def forward(self,x):\n",
    "        x=self.rbflayer(x)\n",
    "        op=self.linearlayer(x)\n",
    "        return op\n",
    "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "input_dim=3*256*256\n",
    "od=2\n",
    "n=50\n",
    "model=RBFNetwork(input_dim,n,od)\n",
    "cri=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "writer=SummaryWriter()\n",
    "dataiter = iter(train_loader)\n",
    "trainloss=[]\n",
    "val_loss=0.0\n",
    "epoch=5\n",
    "for i in range(epoch):\n",
    "    for images,label_idx,labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output=model(images)\n",
    "        #print(label_idx)\n",
    "        loss=cri(output,label_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        val_loss +=cri(output, label_idx).item()\n",
    "\n",
    "    val_loss /= len(train_loader)\n",
    "    writer.add_scalar('loss',val_loss,global_step=epoch)\n",
    "    print(\"epoch \",i+1,loss.item())\n",
    "    trainloss.append(loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7771 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "bs=32\n",
    "ims=(32,32)\n",
    "data=ImageDataGenerator(rescale=1./255)\n",
    "train_gen=data.flow_from_directory(r\"c:\\Users\\maha9\\OneDrive\\Documents\\datasets\\plants\\Apple\\Train\",\n",
    "                                   target_size=ims,\n",
    "                                   batch_size=bs,\n",
    "                                   class_mode=\"categorical\",\n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 196 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maha9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maha9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\maha9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Users\\maha9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\maha9\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "243/243 [==============================] - 7s 25ms/step - loss: 1.3858 - accuracy: 0.2516 - val_loss: 1.3850 - val_accuracy: 0.2602\n",
      "Epoch 2/5\n",
      "243/243 [==============================] - 5s 22ms/step - loss: 1.3851 - accuracy: 0.2578 - val_loss: 1.3846 - val_accuracy: 0.2602\n",
      "Epoch 3/5\n",
      "243/243 [==============================] - 5s 23ms/step - loss: 1.3849 - accuracy: 0.2563 - val_loss: 1.3846 - val_accuracy: 0.2602\n",
      "Epoch 4/5\n",
      "243/243 [==============================] - 6s 23ms/step - loss: 1.3850 - accuracy: 0.2512 - val_loss: 1.3845 - val_accuracy: 0.2602\n",
      "Epoch 5/5\n",
      "243/243 [==============================] - 6s 23ms/step - loss: 1.3849 - accuracy: 0.2518 - val_loss: 1.3845 - val_accuracy: 0.2602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x144921ca810>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras import layers,models\n",
    "test_gen=data.flow_from_directory(r\"c:\\Users\\maha9\\OneDrive\\Documents\\datasets\\plants\\Apple\\Test\",\n",
    "                                   target_size=ims,\n",
    "                                   batch_size=bs,\n",
    "                                   class_mode=\"categorical\",\n",
    "                                   shuffle=False)\n",
    "num_centers=4\n",
    "kmeansdata=[]\n",
    "for i in range(len(train_gen)):\n",
    "    inputs,_=train_gen.next()\n",
    "    kmeansdata.append(inputs.reshape(inputs.shape[0],-1))\n",
    "    import numpy as np\n",
    "kmeansdata=np.concatenate(kmeansdata,axis=0)\n",
    "kmeans=KMeans(n_clusters=num_centers,random_state=42)\n",
    "kmeans.fit(kmeansdata)\n",
    "initial_centers=kmeans.cluster_centers_\n",
    "class RBFLayer(layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        super(RBFLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        centers = initial_centers\n",
    "\n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                       shape=(self.output_dim, input_shape[-1]),\n",
    "                                       initializer=tensorflow.constant_initializer(centers),\n",
    "                                       trainable=True)\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        diff = tensorflow.expand_dims(x, axis=1) - self.centers\n",
    "        norm = tensorflow.norm(diff, axis=-1)\n",
    "        sigma=tensorflow.math.reduce_std(x)\n",
    "        return tensorflow.exp(-0.1 * tensorflow.square(norm)/(2*(sigma**2)))\n",
    "model=models.Sequential([\n",
    "    layers.Flatten(input_shape=(32, 32, 3)), \n",
    "    RBFLayer(4), \n",
    "    layers.Dense(4,activation='softmax')  \n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "tensor_callback=TensorBoard(log_dir=\"./logs\")\n",
    "model.fit(train_gen,epochs=5,validation_data=test_gen,callbacks=[tensor_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir=logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
